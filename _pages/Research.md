---
layout: archive
title: ""
permalink: /Research/
author_profile: true
---

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <td style="padding:20px;width:30%;vertical-align:top">
              <img src='/files/Safe_modular/Mars_exploration.jpg' width="220">
               <br>
                 <img src='/files/Safe_modular/particale.jpg' width="220">
            </td>
            <td style="padding:20px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2109.02791">
                  <papertitle><strong>Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions</strong></papertitle>
              </a>
              <br>
              <strong>Mingyu Cai</strong>, Cristian-Ioan Vasile
              <br>
              <br>
              <a href="https://www.youtube.com/watch?v=fkCyAgx_FWM/">Video</a> /
              <a href="https://arxiv.org/abs/2109.02791">PDF</a>
              <p></p>
              <p>Reinforcement learning (RL) is a promising approach. However, success is limited towards real-world applications, because ensuring safe exploration and facilitating adequate exploitation is a challenge for controlling robotic systems with unknown models and measurement uncertainties. The learning problem becomes even more difficult for complex tasks over continuous state-space and action-space. In this project, we propose a learning-based control framework to satisfy high-level complex task while ensure safe during training. </p>
              <center> <img src='/files/Safe_modular/Modular_architecture.jpg' width="420"> </center>
  </td>
</tbody></table>

<br> <!-- New line --> 
<br> <!-- New line --> 
<br> <!-- New line --> 

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <td style="padding:20px;width:30%;vertical-align:top">
              <img src='/files/Safe_modular/demo1.jpg' width="220">
               <br>
                 <img src='/files/Safe_modular/demo2.jpg' width="220">
            </td>
            <td style="padding:20px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2102.12855.pdf">
                  <papertitle><strong>Modular deep reinforcement learning for continuous motion planning with temporal logic</strong></papertitle>
              </a>
              <br>
              <strong>Mingyu Cai</strong>, Mohammadhosein Hasanbeig, Shaoping Xiao, Alessandro Abate, Zhen Kan
              <br>
              <br>
              <a href="https://github.com/mingyucai/Modular_Deep_RL_E-LDGBA">Demo</a> /
              <a href="https://arxiv.org/pdf/2102.12855.pdf">PDF</a>
              <p></p>
              <p>This project investigates the motion planning of autonomous dynamical systems modeled by Markov decision processes (MDP) with unknown transition probabilities over continuous state and action spaces. Linear temporal logic (LTL) is used to specify high-level tasks over infinite horizon such that the objective is to find controllers satisfying the complex tasks with probabilistic guarantees. A modular deep deterministic policy gradient (DDPG) is then developed to generate such policies over continuous state and action spaces. The performance of our framework is evaluated via an array of OpenAI gym environments. </p>
  </td>
</tbody></table>

<br> <!-- New line --> 
<br> <!-- New line --> 
<br> <!-- New line --> 

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <td style="padding:20px;width:30%;vertical-align:top">
              <img src='/files/Safe_modular/MPC.JPG' width="220">
            </td>
            <td style="padding:20px;width:80%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2007.12123.pdf">
                  <papertitle><strong> Online Motion Planning with Temporal Logic using Model Predictive Control</strong></papertitle>
              </a>
              <br>
              <strong>Mingyu Cai</strong>, Zhiliang Li, Hao Peng, Shaoping Xiao, Zhen Kan
              <br>
              <br>
              <a href="https://www.youtube.com/watch?v=16j6TmVUrTk&t=2s">Video1</a> /
              <a href="https://www.youtube.com/watch?v=RyRnKXDDH5U&t=4s">Video2</a> /
              <a href="https://www.youtube.com/watch?v=S_jfavmFIMo&t=4s">Video3</a> /
              <a href="https://arxiv.org/pdf/2007.12123.pdf">PDF1</a> /
              <a href="https://arxiv.org/pdf/2110.09007.pdf">PDF2</a>
              <p></p>
              <p>This project considers online optimal motion planning of an autonomous agent subject to linear temporal logic (LTL) or Metric Inteval Temporal Logic (MITL) constraints. 
              The environment is dynamic in the sense of containing mobile obstacles and time-varying areas of interest (i.e., time-varying reward and workspace properties) to be visited by the agent. Since user-specified tasks may not be fully realized (i.e., partially infeasible), this work considers hard and soft task-constraints, where hard constraints enforce safety requirement (e.g. avoid obstacles) while soft constraints represent tasks that can be relaxed to not strictly follow user specifications. The motion planning of the agent is to generate policies, in decreasing order of priority, to 1) formally guarantee the satisfaction of safety constraints; 2) mostly satisfy soft constraints (i.e., minimize the violation cost if desired tasks are partially infeasible); and 3) optimize the objective of rewards collection (i.e., visiting dynamic areas of more interests). </p>
  </td>
</tbody></table>
      

              
              


     

