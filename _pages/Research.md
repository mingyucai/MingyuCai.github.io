---
layout: archive
title: ""
permalink: /Research/
author_profile: true
---

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     <tr>
         <td style="padding:20px;width:50%;vertical-align:middle">
           <a href="https://arxiv.org/abs/2109.02791">
               <papertitle><strong>Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions</strong></papertitle>
           </a>
           <br>
           <strong>Mingyu Cai</strong>,
           <a href="https://cristianvasile.com/">Cristian-Ioan Vasile</a>
           <br>
           <em>In submission</em>, 2021.
           <br>
           <a href="https://www.youtube.com/watch?v=fkCyAgx_FWM/">Video</a> /
           <a href="https://arxiv.org/abs/2109.02791">PDF</a>
           <p></p>
           <p>Reinforcement learning (RL) is a promising approach. However, success is limited towards real-world applications, because ensuring safe exploration and facilitating adequate exploitation is a challenge for controlling robotic systems with unknown models and measurement uncertainties. The learning problem becomes even more difficult for complex tasks over continuous state-space and action-space. In this paper, we propose a learning-based control framework consisting of several aspects: (1) we leverage Linear Temporal Logic (LTL) to express complex tasks over an infinite horizons that are translated to a novel automaton structure; (2) we propose an innovative reward scheme for RL-agents with the formal guarantee that global optimal policies maximize the probability of satisfying the LTL specifications; (3) based on a reward shaping technique, we develop a modular policy-gradient architecture exploiting the benefits of the automaton structure to decompose overall tasks and enhance the performance of learned controllers; (4) by incorporating Gaussian Processes (GPs) to estimate the uncertain dynamic systems, we synthesize a model-based safeguard using Exponential Control Barrier Functions (ECBFs) for systems with high-order relative degrees. In addition, we utilize the properties of LTL automata and ECBFs to develop a guiding process to further improve the efficiency of exploration. </p>
<p> <center> <img src='/papers_files/Safety_Critical/Demo.png' width="620"> </center></p>            
     
              
              
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     <tr>
         <td style="padding:20px;width:50%;vertical-align:middle">
           <a href="https://arxiv.org/abs/2109.02791">
               <papertitle><strong>Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions</strong></papertitle>
           </a>
           <br>
           <strong>Mingyu Cai</strong>,
           <a href="https://cristianvasile.com/">Cristian-Ioan Vasile</a>
           <br>
           <em>In submission</em>, 2021.
           <br>
           <a href="https://www.youtube.com/watch?v=fkCyAgx_FWM/">Video</a> /
           <a href="https://arxiv.org/abs/2109.02791">PDF</a>
           <p></p>
           <p>Reinforcement learning (RL) is a promising approach. However, success is limited towards real-world applications, because ensuring safe exploration and facilitating adequate exploitation is a challenge for controlling robotic systems with unknown models and measurement uncertainties. The learning problem becomes even more difficult for complex tasks over continuous state-space and action-space. In this paper, we propose a learning-based control framework consisting of several aspects: (1) we leverage Linear Temporal Logic (LTL) to express complex tasks over an infinite horizons that are translated to a novel automaton structure; (2) we propose an innovative reward scheme for RL-agents with the formal guarantee that global optimal policies maximize the probability of satisfying the LTL specifications; (3) based on a reward shaping technique, we develop a modular policy-gradient architecture exploiting the benefits of the automaton structure to decompose overall tasks and enhance the performance of learned controllers; (4) by incorporating Gaussian Processes (GPs) to estimate the uncertain dynamic systems, we synthesize a model-based safeguard using Exponential Control Barrier Functions (ECBFs) for systems with high-order relative degrees. In addition, we utilize the properties of LTL automata and ECBFs to develop a guiding process to further improve the efficiency of exploration. </p>
<p> <center> <img src='/papers_files/Safety_Critical/Demo.png' width="620"> </center></p>   

     

